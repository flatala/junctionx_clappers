{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Extremist Content Detection Pipeline\n",
    "\n",
    "This pipeline transcribes audio/video files using Whisper and will later analyze the content for extremist material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017faf63",
   "metadata": {},
   "source": [
    "To run: pip install -r ai_pipeline/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import ffmpeg\n",
    "from pathlib import Path\n",
    "import os\n",
    "from typing import Dict, Optional\n",
    "import warnings\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-section",
   "metadata": {},
   "source": [
    "## 2. Load Whisper Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load-model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Whisper model...\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load Whisper model (base model for speed, can use 'small', 'medium', 'large' for better accuracy)\n",
    "print(\"Loading Whisper model...\")\n",
    "model = whisper.load_model(\"base\") \n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processing-section",
   "metadata": {},
   "source": [
    "## 3. Video/Audio Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "convert-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_video_to_audio(video_path: str, output_audio_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert video file to audio (wav format) using ffmpeg.\n",
    "    Args:\n",
    "        video_path: Path to input video file\n",
    "        output_audio_path: Optional path for output audio file\n",
    "    Returns:\n",
    "        Path to the extracted audio file\n",
    "    \"\"\"\n",
    "    video_path = Path(video_path)\n",
    "    audio_path = Path(output_audio_path)\n",
    "    # ensure audio directory exists\n",
    "    audio_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    try:\n",
    "        print(f\"[INFO] Extracting audio from video: {video_path}\")\n",
    "        stream = ffmpeg.input(str(video_path))\n",
    "        stream = ffmpeg.output(stream, str(output_audio_path), acodec='pcm_s16le', ac=1, ar='16k')\n",
    "        ffmpeg.run(stream, overwrite_output=True, capture_stdout=True, capture_stderr=True)\n",
    "        print(f\"[INFO] Converted video to audio: {output_audio_path}\")\n",
    "        return str(output_audio_path)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Error converting video: {e}\")\n",
    "        raise\n",
    "\n",
    "def split_audio_to_patches(audio_path: str, patch_duration_sec: int = 120, overlap_sec: int = 30):\n",
    "    \"\"\"\n",
    "    Split audio into overlapping patches.\n",
    "    Args:\n",
    "        audio_path: Path to audio file\n",
    "        patch_duration_sec: Duration of each patch in seconds (default 2 min)\n",
    "        overlap_sec: Overlap between patches in seconds\n",
    "    Returns:\n",
    "        List of patch file paths\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] Loading audio for patching: {audio_path}\")\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    total_duration = librosa.get_duration(y=y, sr=sr)\n",
    "    print(f\"[INFO] Audio duration: {total_duration:.2f} seconds\")\n",
    "    patch_samples = int(patch_duration_sec * sr)\n",
    "    overlap_samples = int(overlap_sec * sr)\n",
    "    step = patch_samples - overlap_samples\n",
    "    patches = []\n",
    "    for start in range(0, len(y), step):\n",
    "        end = min(start + patch_samples, len(y))\n",
    "        patch_y = y[start:end]\n",
    "        patch_idx = len(patches)\n",
    "        patch_path = Path(audio_path).parent / f\"patch_{patch_idx:03d}.wav\"\n",
    "        sf.write(str(patch_path), patch_y, sr)\n",
    "        print(f\"[INFO] Saved patch {patch_idx}: {patch_path} ({(end-start)/sr:.2f}s)\")\n",
    "        patches.append(str(patch_path))\n",
    "        if end == len(y):\n",
    "            break\n",
    "    return patches\n",
    "\n",
    "def remove_long_silence(audio_path: str, silence_thresh: float = 0.01, min_silence_len: float = 2.0):\n",
    "    \"\"\"\n",
    "    Remove long silent breaks from audio.\n",
    "    Args:\n",
    "        audio_path: Path to audio file\n",
    "        silence_thresh: Amplitude threshold for silence\n",
    "        min_silence_len: Minimum silence length in seconds to remove\n",
    "    Returns:\n",
    "        Path to processed audio file\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] Removing long silences from: {audio_path}\")\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    intervals = librosa.effects.split(y, top_db=40)\n",
    "    processed = []\n",
    "    for start, end in intervals:\n",
    "        segment = y[start:end]\n",
    "        if (end-start)/sr > min_silence_len and np.max(np.abs(segment)) < silence_thresh:\n",
    "            print(f\"[INFO] Skipping long silence: {start/sr:.2f}-{end/sr:.2f}s\")\n",
    "            continue\n",
    "        processed.append(segment)\n",
    "    if processed:\n",
    "        y_out = np.concatenate(processed)\n",
    "    else:\n",
    "        y_out = y\n",
    "    out_path = str(Path(audio_path).with_name(Path(audio_path).stem + '_nosilence.wav'))\n",
    "    sf.write(out_path, y_out, sr)\n",
    "    print(f\"[INFO] Saved audio without long silences: {out_path}\")\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transcribe-section",
   "metadata": {},
   "source": [
    "## 4. Transcription Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "transcribe-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_file(file_path: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Transcribe audio or video file using Whisper.\n",
    "    If video file is provided, it will be converted to audio first.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to audio or video file\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing transcription results:\n",
    "        - text: Full transcription text\n",
    "        - language: Detected language\n",
    "        - segments: List of segments with timestamps and text\n",
    "    \"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not file_path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    # Video extensions that need conversion\n",
    "    video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv', '.webm']\n",
    "    audio_extensions = ['.wav', '.mp3', '.m4a', '.flac', '.ogg']\n",
    "    \n",
    "    # Determine if we need to convert video to audio\n",
    "    if file_path.suffix.lower() in video_extensions:\n",
    "        print(f\"Video file detected. Converting to audio...\")\n",
    "        audio_path = convert_video_to_audio(str(file_path))\n",
    "    elif file_path.suffix.lower() in audio_extensions:\n",
    "        audio_path = str(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_path.suffix}\")\n",
    "    \n",
    "    # Transcribe\n",
    "    print(f\"Transcribing: {audio_path}\")\n",
    "    result = model.transcribe(audio_path)\n",
    "    \n",
    "    # Return structured output\n",
    "    return {\n",
    "        'text': result['text'],\n",
    "        'language': result['language'],\n",
    "        'segments': result['segments']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example-section",
   "metadata": {},
   "source": [
    "## 5. Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09863056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Extracting audio from video: /Users/egor_demin/hack_dataset/hate_videos/hate_video_5.mp4\n",
      "[INFO] Converted video to audio: /Users/egor_demin/hack_dataset/hate_audios/test_audio_5.wav\n",
      "[INFO] Removing long silences from: /Users/egor_demin/hack_dataset/hate_audios/test_audio_5.wav\n",
      "[INFO] Saved audio without long silences: /Users/egor_demin/hack_dataset/hate_audios/test_audio_5_nosilence.wav\n",
      "[INFO] Loading audio for patching: /Users/egor_demin/hack_dataset/hate_audios/test_audio_5_nosilence.wav\n",
      "[INFO] Audio duration: 33.60 seconds\n",
      "[INFO] Saved patch 0: /Users/egor_demin/hack_dataset/hate_audios/patch_000.wav (33.60s)\n",
      "[INFO] Transcribing patch 0: /Users/egor_demin/hack_dataset/hate_audios/patch_000.wav\n",
      "[INFO] Patch 0 detected language: en\n",
      "[INFO] Patch 0 transcription:  Hello, my nigger, my old friend. You've come to rob my house again. Cause you're nigger, soft, blee...\n",
      "\n",
      "[RESULT] Full transcription (first 500 chars):\n",
      "  Hello, my nigger, my old friend. You've come to rob my house again. Cause you're nigger, soft, bleep, bleeping. You're up my house while I'm sleeping. And you're stinking, nigger, smell in my brain. Still remains. I is a transnigger.\n"
     ]
    }
   ],
   "source": [
    "# --- FULL PIPELINE DEMO ---\n",
    "dataset_root = os.getenv(\"DATASET_ROOT\")\n",
    "dataset_path = Path(f\"{dataset_root}/hack_dataset/\")\n",
    "audio_path_output = dataset_path / \"hate_audios\" / \"test_audio_5.wav\"\n",
    "test_video_path = dataset_path / \"hate_videos\" / \"hate_video_5.mp4\"\n",
    "\n",
    "\n",
    "video_path = test_video_path\n",
    "\n",
    "# 1. Convert video to audio\n",
    "raw_audio_path = convert_video_to_audio(video_path, output_audio_path=audio_path_output)\n",
    "\n",
    "# 2. Remove long silences from audio\n",
    "processed_audio_path = remove_long_silence(raw_audio_path)\n",
    "\n",
    "# 3. Split audio into overlapping 2-min patches\n",
    "patches = split_audio_to_patches(processed_audio_path, patch_duration_sec=120, overlap_sec=30)\n",
    "\n",
    "# 4. Transcribe each patch with Whisper\n",
    "all_text = []\n",
    "for i, patch_path in enumerate(patches):\n",
    "    print(f\"[INFO] Transcribing patch {i}: {patch_path}\")\n",
    "    result = model.transcribe(patch_path)\n",
    "    print(f\"[INFO] Patch {i} detected language: {result['language']}\")\n",
    "    print(f\"[INFO] Patch {i} transcription: {result['text'][:100]}...\")\n",
    "    all_text.append(result['text'])\n",
    "\n",
    "# 5. Combine all patch transcriptions\n",
    "full_transcription = '\\n'.join(all_text)\n",
    "print(\"\\n[RESULT] Full transcription (first 500 chars):\\n\", full_transcription[:500])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
