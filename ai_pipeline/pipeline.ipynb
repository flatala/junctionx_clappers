{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Extremist Content Detection Pipeline\n",
    "\n",
    "This pipeline transcribes audio/video files using Whisper and will later analyze the content for extremist material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017faf63",
   "metadata": {},
   "source": [
    "To run: pip install -r ai_pipeline/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import ffmpeg\n",
    "from pathlib import Path\n",
    "import os\n",
    "from typing import Dict, Optional\n",
    "import warnings\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-section",
   "metadata": {},
   "source": [
    "## 2. Load Whisper Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load-model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Whisper model...\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load Whisper model (base model for speed, can use 'small', 'medium', 'large' for better accuracy)\n",
    "print(\"Loading Whisper model...\")\n",
    "model = whisper.load_model(\"base\") \n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processing-section",
   "metadata": {},
   "source": [
    "## 3. Video/Audio Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "convert-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_video_to_audio(video_path: str, output_audio_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert video file to audio (wav format) using ffmpeg.\n",
    "    Args:\n",
    "        video_path: Path to input video file\n",
    "        output_audio_path: Optional path for output audio file\n",
    "    Returns:\n",
    "        Path to the extracted audio file\n",
    "    \"\"\"\n",
    "    video_path = Path(video_path)\n",
    "    audio_path = Path(output_audio_path)\n",
    "    # ensure audio directory exists\n",
    "    audio_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    try:\n",
    "        print(f\"[INFO] Extracting audio from video: {video_path}\")\n",
    "        stream = ffmpeg.input(str(video_path))\n",
    "        stream = ffmpeg.output(stream, str(output_audio_path), acodec='pcm_s16le', ac=1, ar='16k')\n",
    "        ffmpeg.run(stream, overwrite_output=True, capture_stdout=True, capture_stderr=True)\n",
    "        print(f\"[INFO] Converted video to audio: {output_audio_path}\")\n",
    "        return str(output_audio_path)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Error converting video: {e}\")\n",
    "        raise\n",
    "\n",
    "def split_audio_to_patches(audio_path: str, patch_duration_sec: int = 120, overlap_sec: int = 30):\n",
    "    \"\"\"\n",
    "    Split audio into overlapping patches.\n",
    "    Args:\n",
    "        audio_path: Path to audio file\n",
    "        patch_duration_sec: Duration of each patch in seconds (default 2 min)\n",
    "        overlap_sec: Overlap between patches in seconds\n",
    "    Returns:\n",
    "        List of patch file paths\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] Loading audio for patching: {audio_path}\")\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    total_duration = librosa.get_duration(y=y, sr=sr)\n",
    "    print(f\"[INFO] Audio duration: {total_duration:.2f} seconds\")\n",
    "    patch_samples = int(patch_duration_sec * sr)\n",
    "    overlap_samples = int(overlap_sec * sr)\n",
    "    step = patch_samples - overlap_samples\n",
    "    patches = []\n",
    "    for start in range(0, len(y), step):\n",
    "        end = min(start + patch_samples, len(y))\n",
    "        patch_y = y[start:end]\n",
    "        patch_idx = len(patches)\n",
    "        patch_path = Path(audio_path).parent / f\"patch_{patch_idx:03d}.wav\"\n",
    "        sf.write(str(patch_path), patch_y, sr)\n",
    "        print(f\"[INFO] Saved patch {patch_idx}: {patch_path} ({(end-start)/sr:.2f}s)\")\n",
    "        patches.append(str(patch_path))\n",
    "        if end == len(y):\n",
    "            break\n",
    "    return patches\n",
    "\n",
    "def remove_long_silence(audio_path: str, silence_thresh: float = 0.01, min_silence_len: float = 2.0):\n",
    "    \"\"\"\n",
    "    Remove long silent breaks from audio.\n",
    "    Args:\n",
    "        audio_path: Path to audio file\n",
    "        silence_thresh: Amplitude threshold for silence\n",
    "        min_silence_len: Minimum silence length in seconds to remove\n",
    "    Returns:\n",
    "        Path to processed audio file\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] Removing long silences from: {audio_path}\")\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    intervals = librosa.effects.split(y, top_db=40)\n",
    "    processed = []\n",
    "    for start, end in intervals:\n",
    "        segment = y[start:end]\n",
    "        if (end-start)/sr > min_silence_len and np.max(np.abs(segment)) < silence_thresh:\n",
    "            print(f\"[INFO] Skipping long silence: {start/sr:.2f}-{end/sr:.2f}s\")\n",
    "            continue\n",
    "        processed.append(segment)\n",
    "    if processed:\n",
    "        y_out = np.concatenate(processed)\n",
    "    else:\n",
    "        y_out = y\n",
    "    out_path = str(Path(audio_path).with_name(Path(audio_path).stem + '_nosilence.wav'))\n",
    "    sf.write(out_path, y_out, sr)\n",
    "    print(f\"[INFO] Saved audio without long silences: {out_path}\")\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transcribe-section",
   "metadata": {},
   "source": [
    "## 4. Transcription Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff6a2f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_patches(patches, model):\n",
    "    \"\"\"\n",
    "    Transcribe each audio patch with Whisper and format output for later processing.\n",
    "    Args:\n",
    "        patches: list of audio patch file paths\n",
    "        model: loaded Whisper model\n",
    "    Returns:\n",
    "        List of dicts, one per patch, with language and word-level timing info.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    for i, patch_path in enumerate(patches):\n",
    "        print(f\"[INFO] Transcribing patch {i}: {patch_path}\")\n",
    "        result = model.transcribe(patch_path, word_timestamps=True, verbose=False)\n",
    "        lang = result.get('language', 'unknown')\n",
    "        words = []\n",
    "        word_id = 0\n",
    "        for segment in result.get('segments', []):\n",
    "            for word in segment.get('words', []):\n",
    "                words.append({\n",
    "                    'id': word_id,\n",
    "                    'word': word.get('word', '').strip(),\n",
    "                    'start': word.get('start', segment.get('start', None)),\n",
    "                    'end': word.get('end', segment.get('end', None)),\n",
    "                    'probability': word.get('probability', None),\n",
    "                    'phrase_text': segment.get('text', ''),\n",
    "                    'phrase_start': segment.get('start', None),\n",
    "                    'phrase_end': segment.get('end', None)\n",
    "                })\n",
    "                word_id += 1\n",
    "        patch_result = {\n",
    "            'language': lang,\n",
    "            'words': words,\n",
    "            'patch_index': i,\n",
    "            'patch_text': result.get('text', '')\n",
    "        }\n",
    "        print(f\"[INFO] Patch {i} language: {lang}, words: {len(words)}\")\n",
    "        all_results.append(patch_result)\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example-section",
   "metadata": {},
   "source": [
    "## 5. Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09863056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Extracting audio from video: /Users/egor_demin/hack_dataset/hate_videos/hate_video_5.mp4\n",
      "[INFO] Converted video to audio: /Users/egor_demin/hack_dataset/hate_audios/test_audio_5.wav\n",
      "[INFO] Loading audio for patching: /Users/egor_demin/hack_dataset/hate_audios/test_audio_5.wav\n",
      "[INFO] Audio duration: 39.89 seconds\n",
      "[INFO] Saved patch 0: /Users/egor_demin/hack_dataset/hate_audios/patch_000.wav (39.89s)\n",
      "[INFO] Transcribing patch 0: /Users/egor_demin/hack_dataset/hate_audios/patch_000.wav\n",
      "[INFO] Converted video to audio: /Users/egor_demin/hack_dataset/hate_audios/test_audio_5.wav\n",
      "[INFO] Loading audio for patching: /Users/egor_demin/hack_dataset/hate_audios/test_audio_5.wav\n",
      "[INFO] Audio duration: 39.89 seconds\n",
      "[INFO] Saved patch 0: /Users/egor_demin/hack_dataset/hate_audios/patch_000.wav (39.89s)\n",
      "[INFO] Transcribing patch 0: /Users/egor_demin/hack_dataset/hate_audios/patch_000.wav\n",
      "Detected language: English\n",
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3989/3989 [00:03<00:00, 1054.13frames/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Patch 0 language: en, words: 38\n",
      "[INFO] Saved all transcription results to /Users/egor_demin/hack_dataset/hate_audios/test_audio_5_transcription.json\n",
      "[PATCH] \n",
      "          Language: en,\n",
      "          Text: ..., \n",
      "          Words: 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- FULL PIPELINE DEMO ---\n",
    "dataset_root = os.getenv(\"DATASET_ROOT\")\n",
    "dataset_path = Path(f\"{dataset_root}/hack_dataset/\")\n",
    "audio_path_output = dataset_path / \"hate_audios\" / \"test_audio_5.wav\"\n",
    "test_video_path = dataset_path / \"hate_videos\" / \"hate_video_5.mp4\"\n",
    "\n",
    "\n",
    "video_path = test_video_path\n",
    "\n",
    "# 1. Convert video to audio\n",
    "raw_audio_path = convert_video_to_audio(video_path, output_audio_path=audio_path_output)\n",
    "\n",
    "# 2. Remove long silences from audio\n",
    "# processed_audio_path = remove_long_silence(raw_audio_path)\n",
    "# no preprocessing to keep timestamps of the raw audio\n",
    "processed_audio_path = raw_audio_path\n",
    "\n",
    "# 3. Split audio into overlapping 2-min patches\n",
    "patches = split_audio_to_patches(processed_audio_path, patch_duration_sec=120, overlap_sec=31)\n",
    "\n",
    "# 4. Transcribe each patch with Whisper\n",
    "all_results = transcribe_patches(patches, model)\n",
    "\n",
    "# 5. Save all results to a JSON file\n",
    "import json\n",
    "output_json_path = dataset_path / \"hate_audios\" / \"test_audio_5_transcription.json\"\n",
    "with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_results, f, ensure_ascii=False, indent=2)\n",
    "print(f\"[INFO] Saved all transcription results to {output_json_path}\")\n",
    "\n",
    "# 6. Combine all patch transcriptions (optional print)\n",
    "for result in all_results:\n",
    "    print(f\"[PATCH] \\n          Language: {result['language']},\\n          Text: {result.get('full_text','')[:100]}..., \\n          Words: {len(result['words'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0f71d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
